{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_frames_to_record=500 #100000\n",
    "background_chunk=100 #100\n",
    "frame_width=848 #640\n",
    "frame_height=480 #480\n",
    "fps=30 #30\n",
    "preset_json_file=\"d415paramset_848480.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/usr/local/lib')\n",
    "import gzip\n",
    "import shutil\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.colors as mcol\n",
    "import matplotlib.cm as cm\n",
    "import math\n",
    "import imageio\n",
    "import pickle\n",
    "import gc\n",
    "import os\n",
    "abra=os.getcwd()\n",
    "import time\n",
    "import json\n",
    "import pyrealsense2 as rs\n",
    "import gzip\n",
    "import cv2\n",
    "from scipy import stats\n",
    "from scipy import ndimage #for finding com\n",
    "from scipy.misc import imsave\n",
    "from itertools import cycle\n",
    "cycol = cycle('bgrcmk') #here I'm generating a list of colors I can plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"aux-param-autoexposure-setpoint\": \"400\", \"aux-param-colorcorrection1\": \"0.461914\", \"aux-param-colorcorrection10\": \"-0.553711\", \"aux-param-colorcorrection11\": \"-0.553711\", \"aux-param-colorcorrection12\": \"0.0458984\", \"aux-param-colorcorrection2\": \"0.540039\", \"aux-param-colorcorrection3\": \"0.540039\", \"aux-param-colorcorrection4\": \"0.208008\", \"aux-param-colorcorrection5\": \"-0.332031\", \"aux-param-colorcorrection6\": \"-0.212891\", \"aux-param-colorcorrection7\": \"-0.212891\", \"aux-param-colorcorrection8\": \"0.68457\", \"aux-param-colorcorrection9\": \"0.930664\", \"aux-param-depthclampmax\": \"65536\", \"aux-param-depthclampmin\": \"0\", \"aux-param-disparityshift\": \"0\", \"controls-autoexposure-auto\": \"True\", \"controls-autoexposure-manual\": \"33000\", \"controls-color-autoexposure-auto\": \"True\", \"controls-color-autoexposure-manual\": \"166\", \"controls-color-backlight-compensation\": \"0\", \"controls-color-brightness\": \"0\", \"controls-color-contrast\": \"50\", \"controls-color-gain\": \"64\", \"controls-color-gamma\": \"300\", \"controls-color-hue\": \"0\", \"controls-color-power-line-frequency\": \"3\", \"controls-color-saturation\": \"64\", \"controls-color-sharpness\": \"50\", \"controls-color-white-balance-auto\": \"True\", \"controls-color-white-balance-manual\": \"4600\", \"controls-depth-gain\": \"16\", \"controls-depth-white-balance-auto\": \"False\", \"controls-laserpower\": \"150\", \"controls-laserstate\": \"on\", \"ignoreSAD\": \"0\", \"param-autoexposure-setpoint\": \"400\", \"param-censusenablereg-udiameter\": \"9\", \"param-censusenablereg-vdiameter\": \"9\", \"param-censususize\": \"9\", \"param-censusvsize\": \"9\", \"param-depthclampmax\": \"65536\", \"param-depthclampmin\": \"0\", \"param-depthunits\": \"1000\", \"param-disableraucolor\": \"0\", \"param-disablesadcolor\": \"0\", \"param-disablesadnormalize\": \"0\", \"param-disablesloleftcolor\": \"0\", \"param-disableslorightcolor\": \"0\", \"param-disparitymode\": \"0\", \"param-disparityshift\": \"0\", \"param-lambdaad\": \"800\", \"param-lambdacensus\": \"26\", \"param-leftrightthreshold\": \"24\", \"param-maxscorethreshb\": \"2047\", \"param-medianthreshold\": \"500\", \"param-minscorethresha\": \"1\", \"param-neighborthresh\": \"7\", \"param-raumine\": \"1\", \"param-rauminn\": \"1\", \"param-rauminnssum\": \"3\", \"param-raumins\": \"1\", \"param-rauminw\": \"1\", \"param-rauminwesum\": \"3\", \"param-regioncolorthresholdb\": \"0.0499022\", \"param-regioncolorthresholdg\": \"0.0499022\", \"param-regioncolorthresholdr\": \"0.0499022\", \"param-regionshrinku\": \"3\", \"param-regionshrinkv\": \"1\", \"param-robbinsmonrodecrement\": \"10\", \"param-robbinsmonroincrement\": \"10\", \"param-rsmdiffthreshold\": \"4\", \"param-rsmrauslodiffthreshold\": \"1\", \"param-rsmremovethreshold\": \"0.375\", \"param-scanlineedgetaub\": \"72\", \"param-scanlineedgetaug\": \"72\", \"param-scanlineedgetaur\": \"72\", \"param-scanlinep1\": \"60\", \"param-scanlinep1onediscon\": \"105\", \"param-scanlinep1twodiscon\": \"70\", \"param-scanlinep2\": \"342\", \"param-scanlinep2onediscon\": \"190\", \"param-scanlinep2twodiscon\": \"130\", \"param-secondpeakdelta\": \"325\", \"param-texturecountthresh\": \"0\", \"param-texturedifferencethresh\": \"0\", \"param-usersm\": \"1\", \"param-zunits\": \"1000\", \"stream-depth-format\": \"Z16\", \"stream-fps\": \"90\", \"stream-height\": \"480\", \"stream-width\": \"640\"}\n"
     ]
    }
   ],
   "source": [
    "  with open(preset_json_file) as json_data: #Json file to be loaded #NOTE im using default NOT the json that I just created above\n",
    "    starting_params = json.load(json_data)\n",
    "    starting_params=str(starting_params).replace(\"'\", '\\\"') #IF YOU DON't DO THIS IT WILL NOT WORK with the librealsense sdk\n",
    "    print(starting_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "415.46185302734375\n",
      "Depth Scale is:  0.0010000000474974513\n"
     ]
    }
   ],
   "source": [
    "#https://github.com/IntelRealSense/librealsense/blob/master/wrappers/python/examples/align-depth2color.py\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = rs.pipeline()\n",
    "\n",
    "#Create a config and configure the pipeline to stream\n",
    "#  different resolutions of color and depth streams\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.depth, frame_width, frame_height, rs.format.z16, 30)\n",
    "config.enable_stream(rs.stream.color, 1280, 720, rs.format.bgr8, 30)\n",
    "\n",
    "# Start streaming\n",
    "profile = pipeline.start(config)\n",
    "stream_profile=profile.get_stream(rs.stream.depth)\n",
    "# getting instrinsic parameters https://github.com/IntelRealSense/librealsense/issues/869\n",
    "intrinsics = stream_profile.as_video_stream_profile().get_intrinsics() # Downcast to video_stream_profile and fetch intrinsics\n",
    "print(intrinsics.ppx)\n",
    "\n",
    "# Getting the depth sensor's depth scale (see rs-align example for explanation)\n",
    "depth_sensor = profile.get_device().first_depth_sensor()\n",
    "depth_scale = depth_sensor.get_depth_scale()\n",
    "print(\"Depth Scale is: \" , depth_scale)\n",
    "\n",
    "# We will be removing the background of objects more than\n",
    "#  clipping_distance_in_meters meters away\n",
    "#clipping_distance_in_meters = 2 #1 meter\n",
    "#clipping_distance = clipping_distance_in_meters / depth_scale\n",
    "\n",
    "# Create an align object\n",
    "# rs.align allows us to perform alignment of depth frames to others frames\n",
    "# The \"align_to\" is the stream type to which we plan to align depth frames.\n",
    "align_to = rs.stream.color #was previously aligned to color\n",
    "align = rs.align(align_to)\n",
    "count=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please insert calibration object and enter YYMMDD_exp######  181031_mouse_two_cherkboards_1\n",
      "please insert object of interestmouse\n"
     ]
    }
   ],
   "source": [
    "# Streaming loop\n",
    "\n",
    "with open('pythondepthdata.bin','ab') as depthbinarydata, open('pythoncolordata.bin', 'ab') as colorbinarydata: #While the file called \"pythonedpthdata.bin\" is open for appending binary, \n",
    "    try:\n",
    "        while count<number_of_frames_to_record:\n",
    "            # Get frameset of color and depth\n",
    "            frames = pipeline.wait_for_frames()\n",
    "            # frames.get_depth_frame() is a 640x360 depth image\n",
    "\n",
    "            # Align the depth frame to color frame\n",
    "            aligned_frames = align.process(frames)\n",
    "\n",
    "            # Get aligned frames\n",
    "            aligned_depth_frame = aligned_frames.get_depth_frame() # aligned_depth_frame is a 640x480 depth image\n",
    "            color_frame = aligned_frames.get_color_frame()\n",
    "\n",
    "            # Validate that both frames are valid\n",
    "            if not aligned_depth_frame or not color_frame:\n",
    "                continue\n",
    "\n",
    "            depth_image = np.asanyarray(aligned_depth_frame.get_data())\n",
    "            depth_image.tofile(depthbinarydata) #save data as binarydata (specified by the with open as line)\n",
    "\n",
    "            color_image = np.asanyarray(color_frame.get_data())\n",
    "            color_image.tofile(colorbinarydata) #save data as binarydata (specified by the with open as line)\n",
    "            \n",
    "            if count==background_chunk:\n",
    "                output=input(\"please insert calibration object and enter YYMMDD_exp######  \")\n",
    "            if count==background_chunk+50:\n",
    "                input(\"please insert object of interest\")\n",
    "            \n",
    "            with open('depth_ts.txt', 'a') as ts:\n",
    "                ts.write('%s\\n' %rs.frame.get_timestamp(frames)) #write time stamp float as a string to a new line\n",
    "            \n",
    "\n",
    "            # Remove background - Set pixels further than clipping_distance to grey\n",
    "            #grey_color = 153\n",
    "            depth_image_3d = np.dstack((depth_image,depth_image,depth_image)) #depth image is 1 channel, color is 3 channels\n",
    "            #bg_removed = np.where((depth_image_3d > clipping_distance) | (depth_image_3d <= 0), grey_color, color_image)\n",
    "\n",
    "            # Render images\n",
    "            depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.13), cv2.COLORMAP_JET)\n",
    "            images = np.hstack((color_image, depth_colormap))\n",
    "            count=count+1;\n",
    "    finally:\n",
    "        pipeline.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "width: 848, height: 480, ppx: 415.462, ppy: 232.335, fx: 633.362, fy: 633.362, model: Brown Conrady, coeffs: [0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "intr=print(intrinsics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(output)\n",
    "shutil.move('pythondepthdata.bin',output)\n",
    "shutil.move('pythoncolordata.bin',output)\n",
    "shutil.move('depth_ts.txt',output)\n",
    "with open(abra + '/' + output + '/' + output + '_variables', 'wb') as variables:\n",
    "    pickle.dump([number_of_frames_to_record,frame_height,frame_width,fps,background_chunk,intr],variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(output)\n",
    "os.mkdir(output+'_depth_frames')\n",
    "os.mkdir(output+'_color_frames')\n",
    "os.mkdir(output+'_stacked_frames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(output+'_variables', 'rb') as variables:\n",
    "    number_of_frames,frame_height,frame_width,fps,background_chunk,intrinsics = pickle.load(variables) #added intrisincs\n",
    "    \n",
    "exp_numb=0\n",
    "box_size=192\n",
    "stringency=10 #10 for d415 50 for sr300\n",
    "grand_frame=int(exp_numb*(number_of_frames-background_chunk))\n",
    "min_range=0\n",
    "max_range=300 #1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_height=720\n",
    "frame_width=1280\n",
    "depthframes = np.fromfile('pythondepthdata.bin', dtype=np.uint16).reshape(-1, frame_height, frame_width)\n",
    "depth_background_frames = depthframes[0:background_chunk]\n",
    "depthframes=depthframes[background_chunk:number_of_frames]\n",
    "depthbackgroundmean=np.median(depth_background_frames, axis=0) #may be better to use median\n",
    "depthbackgroundvariance=np.var(depth_background_frames, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "colorframes = np.fromfile('pythoncolordata.bin', dtype=np.uint8).reshape(-1, frame_height, frame_width,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndepth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)\\nimages = np.hstack((color_image, depth_colormap))\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "images = np.hstack((color_image, depth_colormap))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "###get the depth and color image of the calibration object\n",
    "current_frame=40\n",
    "depth_frame_of_interest=depthframes[current_frame] \n",
    "color_frame_of_interest=colorframes[current_frame+background_chunk] \n",
    "#color\n",
    "#color_frame_of_interest=np.dot(color_frame_of_interest[...,:3], [.299, .587, .114])\n",
    "#color_frame_of_interest=np.subtract(color_frame_of_interest,(colorbackgroundmean))\n",
    "#DEPTH BACKGROUND SUBTRACTION#####################################################\n",
    "#Background subtraction with no filters\n",
    "    \n",
    "#IF PIXEL ISN'T SIGNIFICANTLY DIFFERENT FROM THE BACKGROUND THEN DROP IT\n",
    "backgroundmultiplier=np.abs(depth_frame_of_interest - depthbackgroundmean) - (stringency*np.sqrt(depthbackgroundvariance)) \n",
    "    \n",
    "\n",
    "#WE EXPECT POSITIVE VALUES FOR HIGH DIFFERENCES make them stay by multiplying by 1\n",
    "backgroundmultiplier[backgroundmultiplier>0]=1\n",
    "#WE EXPECT NEGATIVE VALUES FOR SMALL OR NO DIFFERENCE probably just noise, get rid of them by multiplying by 0\n",
    "backgroundmultiplier[backgroundmultiplier<=0]=0\n",
    "       \n",
    "# SUBTRACT BACKGROUND BY MULTIPLYING INSIGNIFIACNT FRAMES BY ZERO\n",
    "depth_frame_of_interest_backgroundsubtracted=(np.multiply(depth_frame_of_interest,backgroundmultiplier)).astype('uint8') #IMWRITE TAKES UINT8\n",
    "depth_frame_of_interest_backgroundsubtracted[depth_frame_of_interest_backgroundsubtracted>max_range]=0\n",
    "depth_frame_of_interest_backgroundsubtracted[depth_frame_of_interest_backgroundsubtracted<=min_range]=0\n",
    "'''\n",
    "\n",
    "'''\n",
    "depth_image=depth_frame_of_interest\n",
    "color_image=color_frame_of_interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[594., 516.],\n",
       "       [593., 483.],\n",
       "       [589., 451.],\n",
       "       [586., 418.],\n",
       "       [584., 385.],\n",
       "       [579., 355.],\n",
       "       [576., 325.],\n",
       "       [571., 292.],\n",
       "       [624., 505.],\n",
       "       [618., 477.],\n",
       "       [615., 446.],\n",
       "       [613., 415.],\n",
       "       [610., 386.],\n",
       "       [606., 354.],\n",
       "       [601., 327.],\n",
       "       [600., 295.],\n",
       "       [907., 303.],\n",
       "       [912., 331.],\n",
       "       [914., 360.],\n",
       "       [919., 388.],\n",
       "       [926., 420.],\n",
       "       [930., 450.],\n",
       "       [878., 307.],\n",
       "       [881., 335.],\n",
       "       [886., 365.],\n",
       "       [892., 394.],\n",
       "       [894., 424.],\n",
       "       [900., 455.],\n",
       "       [849., 309.],\n",
       "       [853., 340.],\n",
       "       [857., 368.],\n",
       "       [861., 398.],\n",
       "       [865., 428.],\n",
       "       [870., 456.],\n",
       "       [821., 315.],\n",
       "       [821., 341.],\n",
       "       [826., 371.],\n",
       "       [833., 398.],\n",
       "       [837., 431.],\n",
       "       [839., 459.],\n",
       "       [792., 318.],\n",
       "       [793., 344.],\n",
       "       [798., 376.],\n",
       "       [803., 405.],\n",
       "       [806., 433.],\n",
       "       [812., 463.],\n",
       "       [763., 323.],\n",
       "       [767., 351.],\n",
       "       [767., 378.],\n",
       "       [773., 407.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_calib_points=50\n",
    "plt.figure(figsize=(40,20))\n",
    "plt.imshow(color_image)\n",
    "realpoints=np.rint(plt.ginput(num_calib_points,timeout=0))\n",
    "virtualpoints=np.rint(plt.ginput(num_calib_points,timeout=0))\n",
    "np.rint(realpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "twod_realpoints=realpoints.astype(int)\n",
    "twod_virtualpoints=virtualpoints.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "threed_realpoints=np.zeros((num_calib_points,3),dtype=int)\n",
    "threed_virtualpoints=np.zeros((num_calib_points,3),dtype=int)\n",
    "\n",
    "for i in range(num_calib_points):\n",
    "    rx=twod_realpoints[i][0]\n",
    "    ry=twod_realpoints[i][1]\n",
    "    threed_realpoints[i]=[twod_realpoints[i][1],twod_realpoints[i][0],depth_image[ry,rx]]\n",
    "    \n",
    "for i in range(num_calib_points):\n",
    "    vx=twod_virtualpoints[i][0]\n",
    "    vy=twod_virtualpoints[i][1]\n",
    "    threed_virtualpoints[i]=[twod_virtualpoints[i][1],twod_virtualpoints[i][0],depth_image[vy,vx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[516, 594, 617],\n",
       "       [483, 593, 620],\n",
       "       [451, 589, 620],\n",
       "       [418, 586, 622],\n",
       "       [385, 584, 626],\n",
       "       [355, 579, 626],\n",
       "       [325, 576, 626],\n",
       "       [292, 571, 621],\n",
       "       [505, 624, 660],\n",
       "       [477, 618, 658],\n",
       "       [446, 615, 660],\n",
       "       [415, 613, 660],\n",
       "       [386, 610, 661],\n",
       "       [354, 606, 669],\n",
       "       [327, 601, 665],\n",
       "       [295, 600, 668],\n",
       "       [303, 907, 686],\n",
       "       [331, 912, 684],\n",
       "       [360, 914, 682],\n",
       "       [388, 919, 683],\n",
       "       [420, 926, 681],\n",
       "       [450, 930, 681],\n",
       "       [307, 878, 690],\n",
       "       [335, 881, 686],\n",
       "       [365, 886, 683],\n",
       "       [394, 892, 683],\n",
       "       [424, 894, 682],\n",
       "       [455, 900, 681],\n",
       "       [309, 849, 689],\n",
       "       [340, 853, 686],\n",
       "       [368, 857, 686],\n",
       "       [398, 861, 683],\n",
       "       [428, 865, 683],\n",
       "       [456, 870, 681],\n",
       "       [315, 821, 686],\n",
       "       [341, 821, 685],\n",
       "       [371, 826, 684],\n",
       "       [398, 833, 683],\n",
       "       [431, 837, 682],\n",
       "       [459, 839, 681],\n",
       "       [318, 792, 686],\n",
       "       [344, 793, 686],\n",
       "       [376, 798, 684],\n",
       "       [405, 803, 683],\n",
       "       [433, 806, 681],\n",
       "       [463, 812, 683],\n",
       "       [323, 763, 684],\n",
       "       [351, 767, 686],\n",
       "       [378, 767, 680],\n",
       "       [407, 773, 681]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threed_realpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 408, 1185,  877],\n",
       "       [ 388, 1181,  877],\n",
       "       [ 365, 1176,  877],\n",
       "       [ 342, 1174,  877],\n",
       "       [ 320, 1169,  882],\n",
       "       [ 297, 1164,  884],\n",
       "       [ 275, 1160,  884],\n",
       "       [ 252, 1159,  874],\n",
       "       [ 414, 1136,  886],\n",
       "       [ 390, 1132,  883],\n",
       "       [ 370, 1129,  886],\n",
       "       [ 348, 1128,  884],\n",
       "       [ 325, 1123,  885],\n",
       "       [ 305, 1119,  884],\n",
       "       [ 282, 1114,  886],\n",
       "       [ 260, 1113,  881],\n",
       "       [ 292, 1023,  726],\n",
       "       [ 320, 1030,  731],\n",
       "       [ 347, 1032,  727],\n",
       "       [ 374, 1037,  724],\n",
       "       [ 403, 1040,  723],\n",
       "       [ 430, 1047,  727],\n",
       "       [ 292, 1032,  745],\n",
       "       [ 319, 1036,  743],\n",
       "       [ 344, 1042,  745],\n",
       "       [ 372, 1046,  745],\n",
       "       [ 398, 1050,  744],\n",
       "       [ 426, 1053,  739],\n",
       "       [ 293, 1040,  761],\n",
       "       [ 317, 1045,  758],\n",
       "       [ 345, 1050,  761],\n",
       "       [ 373, 1055,  763],\n",
       "       [ 398, 1059,  762],\n",
       "       [ 423, 1062,  759],\n",
       "       [ 295, 1048,  779],\n",
       "       [ 318, 1052,  776],\n",
       "       [ 345, 1058,  776],\n",
       "       [ 368, 1062,  778],\n",
       "       [ 393, 1066,  777],\n",
       "       [ 420, 1069,  776],\n",
       "       [ 292, 1054,  792],\n",
       "       [ 317, 1059,  791],\n",
       "       [ 341, 1064,  793],\n",
       "       [ 366, 1069,  794],\n",
       "       [ 391, 1072,  789],\n",
       "       [ 416, 1074,  787],\n",
       "       [ 292, 1061,  807],\n",
       "       [ 316, 1066,  810],\n",
       "       [ 342, 1072,  808],\n",
       "       [ 365, 1076,  809]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threed_virtualpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for affine transformation using 4 points from each space\n",
    "def solve_affine( space1, space2 ):\n",
    "    x = np.transpose(np.matrix(space1))\n",
    "    y = np.transpose(np.matrix(space2))\n",
    "    # add ones on the bottom of x and y\n",
    "    x = np.vstack((x,[np.ones(len(space1),dtype=int)]))\n",
    "    y = np.vstack((y,[np.ones(len(space2),dtype=int)]))\n",
    "    # solve for A2\n",
    "    A2 = y * x.I\n",
    "    # return function that takes input x and transforms it\n",
    "    # don't need to return the 4th row\n",
    "    return lambda x: (A2*np.vstack((np.matrix(x).reshape(3,1),1)))[0:3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "virtual2realaffine=solve_affine(threed_virtualpoints,threed_realpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[408.14755394],\n",
       "        [769.12008505],\n",
       "        [680.46947223]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#IM ABLE TO GET PRETTY DAMN CLOSE TO THE ONE POINT THAT DID NOT GO INTO THE AFFINE TRANSFORM!!!\n",
    "virtual2realaffine(threed_virtualpoints[num_calib_points-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe325125f28>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "plt.imshow(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe325125588>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.imshow(depth_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[  27.45712132],\n",
       "        [1962.20460109],\n",
       "        [ 434.95666305]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity check\n",
    "x=1012\n",
    "y=210\n",
    "virtual2realaffine([y,x,depth_image[y,x]])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe3251d2358>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.imshow(color_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "###new depth and color images are of the object of interest.\n",
    "stringency=15\n",
    "current_frame=299\n",
    "depth_frame_of_interest=depthframes[current_frame] \n",
    "color_frame_of_interest=colorframes[current_frame+background_chunk] \n",
    "#color\n",
    "#color_frame_of_interest=np.dot(color_frame_of_interest[...,:3], [.299, .587, .114])\n",
    "#color_frame_of_interest=np.subtract(color_frame_of_interest,(colorbackgroundmean))\n",
    "#DEPTH BACKGROUND SUBTRACTION#####################################################\n",
    "#Background subtraction with no filters\n",
    "    \n",
    "#IF PIXEL ISN'T SIGNIFICANTLY DIFFERENT FROM THE BACKGROUND THEN DROP IT\n",
    "backgroundmultiplier=np.abs(depth_frame_of_interest - depthbackgroundmean) - (stringency*np.sqrt(depthbackgroundvariance)) \n",
    "    \n",
    "\n",
    "#WE EXPECT POSITIVE VALUES FOR HIGH DIFFERENCES make them stay by multiplying by 1\n",
    "backgroundmultiplier[backgroundmultiplier>0]=1\n",
    "#WE EXPECT NEGATIVE VALUES FOR SMALL OR NO DIFFERENCE probably just noise, get rid of them by multiplying by 0\n",
    "backgroundmultiplier[backgroundmultiplier<=0]=0\n",
    "       \n",
    "# SUBTRACT BACKGROUND BY MULTIPLYING INSIGNIFIACNT FRAMES BY ZERO\n",
    "depth_frame_of_interest_backgroundsubtracted=(np.multiply(depth_frame_of_interest,backgroundmultiplier)).astype('uint8') #IMWRITE TAKES UINT8\n",
    "depth_frame_of_interest_backgroundsubtracted[depth_frame_of_interest_backgroundsubtracted>max_range]=0\n",
    "depth_frame_of_interest_backgroundsubtracted[depth_frame_of_interest_backgroundsubtracted<=min_range]=0\n",
    "'''\n",
    "\n",
    "'''\n",
    "depth_image=depth_frame_of_interest\n",
    "color_image=color_frame_of_interest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyntcloud import PyntCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NO AFFINE TRANSFORM\n",
    "\n",
    "positions=np.zeros(((1280*720),3), dtype=int)\n",
    "colors=np.zeros(((1280*720),3), dtype=int)\n",
    "\n",
    "index=0\n",
    "\n",
    "for i in range(1280):\n",
    "    for j in range(720):\n",
    "        \n",
    "        positions[index,0]=i\n",
    "        positions[index,1]=j\n",
    "        positions[index,2]=-1*depth_image[j,i] #height,width\n",
    "        colors[index,0]=color_image[j,i][0]\n",
    "        colors[index,1]=color_image[j,i][1]\n",
    "        colors[index,2]=color_image[j,i][2]\n",
    "        \n",
    "        if depth_frame_of_interest_backgroundsubtracted[j,i]==0: #==0\n",
    "            positions[index,0]=0\n",
    "            positions[index,1]=0\n",
    "            positions[index,2]=0\n",
    "            colors[index,0]=0\n",
    "            colors[index,1]=0\n",
    "            colors[index,2]=0\n",
    "        \n",
    "        index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply affine transform to pixels higher than a certain threshold (where the mirror is)\n",
    "\n",
    "affpositions=np.zeros(((1280*720),3), dtype=int)\n",
    "affcolors=np.zeros(((1280*720),3), dtype=int)\n",
    "\n",
    "index=0\n",
    "\n",
    "for i in range(1280):\n",
    "    for j in range(720):\n",
    "        \n",
    "        if i>1000 and j>100 and j<500: #i looked at the images by eye and found the mirror to be above this point. If I set J to a large number I create \"reverse\" world where everything is affine transformed\n",
    "            affpositions[index,0]=virtual2realaffine([j,i,depth_image[j,i]])[1]\n",
    "            affpositions[index,1]=virtual2realaffine([j,i,depth_image[j,i]])[0]\n",
    "            affpositions[index,2]=virtual2realaffine([j,i,depth_image[j,i]])[2]*-1 #if using raw image then *-1 but if using background subtracted image then leave as b\n",
    "        else:\n",
    "            affpositions[index,0]=i\n",
    "            affpositions[index,1]=j\n",
    "            affpositions[index,2]=-1*depth_image[j,i] #height,width\n",
    "            \n",
    "        affcolors[index,0]=color_image[j,i][0]\n",
    "        affcolors[index,1]=color_image[j,i][1]\n",
    "        affcolors[index,2]=color_image[j,i][2]\n",
    "        \n",
    "        if depth_frame_of_interest_backgroundsubtracted[j,i]==0: #==0\n",
    "            affpositions[index,0]=0\n",
    "            affpositions[index,1]=0\n",
    "            affpositions[index,2]=0\n",
    "            affcolors[index,0]=0\n",
    "            affcolors[index,1]=0\n",
    "            affcolors[index,2]=0\n",
    "            \n",
    "        \n",
    "        index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = pd.DataFrame(\n",
    "    positions.astype(np.float32), \n",
    "    columns=['x', 'y', 'z'])\n",
    "\n",
    "points['red']=colors[:,0].astype(np.uint8)\n",
    "points['blue']=colors[:,2].astype(np.uint8)\n",
    "points['green']=colors[:,1].astype(np.uint8)\n",
    "\n",
    "cloud = PyntCloud(points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "affpoints = pd.DataFrame(\n",
    "    affpositions.astype(np.float32), \n",
    "    columns=['x', 'y', 'z'])\n",
    "\n",
    "affpoints['red']=affcolors[:,0].astype(np.uint8)\n",
    "affpoints['blue']=affcolors[:,2].astype(np.uint8)\n",
    "affpoints['green']=affcolors[:,1].astype(np.uint8)\n",
    "\n",
    "affcloud = PyntCloud(affpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44bb4528dc514c7e896e00e2bce4d502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(aspect=1.6, fov=90.0, position=(4.882261276245117, 720.3687620162964, -5.982…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f470f15510884f6897369a208a54c237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value='Point size:'), FloatSlider(value=6.0, max=60.0, step=0.06), Label(value='Backgroun…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cloud.plot(initial_point_size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2b787895d2d4c8ab1702de3e0e2052f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(aspect=1.6, fov=90.0, position=(4.241404056549072, 720.4446876049042, -5.708…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bda4f2963d73444abdc3d73e0ca01918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value='Point size:'), FloatSlider(value=6.0, max=60.0, step=0.06), Label(value='Backgroun…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "affcloud.plot(initial_point_size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
